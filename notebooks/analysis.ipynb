{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-LLM Collaborative Debate System - Analysis\n",
    "\n",
    "This notebook visualizes the experiment results from the debate system.\n",
    "\n",
    "## Metrics Analyzed:\n",
    "1. **Overall Accuracy** - How often the final answer is correct\n",
    "2. **Judge Accuracy vs Majority Vote** - Compare judge selection vs consensus\n",
    "3. **Collaboration Bonus** - Did refinement improve answers?\n",
    "4. **Accuracy by Category** - Performance across Math, Logic, Physics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load results from `data/results_log.json`. If missing or empty, generate dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = Path(\"../data/results_log.json\")\n",
    "\n",
    "def generate_dummy_data() -> list[dict]:\n",
    "    \"\"\"Generate dummy data for visualization testing.\"\"\"\n",
    "    dummy_results = [\n",
    "        {\n",
    "            \"problem_id\": 1,\n",
    "            \"category\": \"Logic\",\n",
    "            \"difficulty\": \"Hard\",\n",
    "            \"question\": \"Sample logic puzzle\",\n",
    "            \"ground_truth\": \"Answer A\",\n",
    "            \"judge_id\": \"C\",\n",
    "            \"solver_ids\": [\"A\", \"B\", \"D\"],\n",
    "            \"initial_solutions\": {\n",
    "                \"A\": {\"final_answer\": \"Answer A\"},\n",
    "                \"B\": {\"final_answer\": \"Answer B\"},\n",
    "                \"D\": {\"final_answer\": \"Answer A\"},\n",
    "            },\n",
    "            \"refined_solutions\": {\n",
    "                \"A\": {\"final_answer\": \"Answer A\"},\n",
    "                \"B\": {\"final_answer\": \"Answer A\"},\n",
    "                \"D\": {\"final_answer\": \"Answer A\"},\n",
    "            },\n",
    "            \"verdict\": {\"best_solver_id\": \"A\", \"final_answer_to_user\": \"Answer A\"},\n",
    "            \"evaluation\": {\"is_correct\": True, \"reasoning\": \"Correct\"},\n",
    "        },\n",
    "        {\n",
    "            \"problem_id\": 2,\n",
    "            \"category\": \"Math\",\n",
    "            \"difficulty\": \"Medium\",\n",
    "            \"question\": \"Sample math problem\",\n",
    "            \"ground_truth\": \"42\",\n",
    "            \"judge_id\": \"A\",\n",
    "            \"solver_ids\": [\"B\", \"C\", \"D\"],\n",
    "            \"initial_solutions\": {\n",
    "                \"B\": {\"final_answer\": \"42\"},\n",
    "                \"C\": {\"final_answer\": \"41\"},\n",
    "                \"D\": {\"final_answer\": \"42\"},\n",
    "            },\n",
    "            \"refined_solutions\": {\n",
    "                \"B\": {\"final_answer\": \"42\"},\n",
    "                \"C\": {\"final_answer\": \"42\"},\n",
    "                \"D\": {\"final_answer\": \"42\"},\n",
    "            },\n",
    "            \"verdict\": {\"best_solver_id\": \"B\", \"final_answer_to_user\": \"42\"},\n",
    "            \"evaluation\": {\"is_correct\": True, \"reasoning\": \"Correct\"},\n",
    "        },\n",
    "        {\n",
    "            \"problem_id\": 3,\n",
    "            \"category\": \"Physics\",\n",
    "            \"difficulty\": \"Medium\",\n",
    "            \"question\": \"Sample physics problem\",\n",
    "            \"ground_truth\": \"51.3\",\n",
    "            \"judge_id\": \"D\",\n",
    "            \"solver_ids\": [\"A\", \"B\", \"C\"],\n",
    "            \"initial_solutions\": {\n",
    "                \"A\": {\"final_answer\": \"50.0\"},\n",
    "                \"B\": {\"final_answer\": \"51.3\"},\n",
    "                \"C\": {\"final_answer\": \"52.0\"},\n",
    "            },\n",
    "            \"refined_solutions\": {\n",
    "                \"A\": {\"final_answer\": \"51.0\"},\n",
    "                \"B\": {\"final_answer\": \"51.3\"},\n",
    "                \"C\": {\"final_answer\": \"51.3\"},\n",
    "            },\n",
    "            \"verdict\": {\"best_solver_id\": \"B\", \"final_answer_to_user\": \"51.3\"},\n",
    "            \"evaluation\": {\"is_correct\": True, \"reasoning\": \"Correct\"},\n",
    "        },\n",
    "        {\n",
    "            \"problem_id\": 4,\n",
    "            \"category\": \"Math\",\n",
    "            \"difficulty\": \"Hard\",\n",
    "            \"question\": \"Sample hard math problem\",\n",
    "            \"ground_truth\": \"1024\",\n",
    "            \"judge_id\": \"B\",\n",
    "            \"solver_ids\": [\"A\", \"C\", \"D\"],\n",
    "            \"initial_solutions\": {\n",
    "                \"A\": {\"final_answer\": \"1000\"},\n",
    "                \"C\": {\"final_answer\": \"1024\"},\n",
    "                \"D\": {\"final_answer\": \"1000\"},\n",
    "            },\n",
    "            \"refined_solutions\": {\n",
    "                \"A\": {\"final_answer\": \"1000\"},\n",
    "                \"C\": {\"final_answer\": \"1024\"},\n",
    "                \"D\": {\"final_answer\": \"1024\"},\n",
    "            },\n",
    "            \"verdict\": {\"best_solver_id\": \"A\", \"final_answer_to_user\": \"1000\"},\n",
    "            \"evaluation\": {\"is_correct\": False, \"reasoning\": \"Incorrect\"},\n",
    "        },\n",
    "        {\n",
    "            \"problem_id\": 5,\n",
    "            \"category\": \"Logic\",\n",
    "            \"difficulty\": \"Hard\",\n",
    "            \"question\": \"Sample hard logic problem\",\n",
    "            \"ground_truth\": \"July 16\",\n",
    "            \"judge_id\": \"A\",\n",
    "            \"solver_ids\": [\"B\", \"C\", \"D\"],\n",
    "            \"initial_solutions\": {\n",
    "                \"B\": {\"final_answer\": \"July 16\"},\n",
    "                \"C\": {\"final_answer\": \"Aug 17\"},\n",
    "                \"D\": {\"final_answer\": \"July 14\"},\n",
    "            },\n",
    "            \"refined_solutions\": {\n",
    "                \"B\": {\"final_answer\": \"July 16\"},\n",
    "                \"C\": {\"final_answer\": \"July 16\"},\n",
    "                \"D\": {\"final_answer\": \"July 16\"},\n",
    "            },\n",
    "            \"verdict\": {\"best_solver_id\": \"B\", \"final_answer_to_user\": \"July 16\"},\n",
    "            \"evaluation\": {\"is_correct\": True, \"reasoning\": \"Correct\"},\n",
    "        },\n",
    "    ]\n",
    "    return dummy_results\n",
    "\n",
    "\n",
    "def load_results() -> list[dict]:\n",
    "    \"\"\"Load results from JSON file or return dummy data.\"\"\"\n",
    "    if RESULTS_PATH.exists():\n",
    "        with open(RESULTS_PATH, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            if data and len(data) > 0:\n",
    "                # Filter out error entries\n",
    "                valid_data = [r for r in data if \"evaluation\" in r]\n",
    "                if valid_data:\n",
    "                    print(f\"Loaded {len(valid_data)} results from {RESULTS_PATH}\")\n",
    "                    return valid_data\n",
    "    \n",
    "    print(\"No valid results found. Using dummy data for visualization.\")\n",
    "    return generate_dummy_data()\n",
    "\n",
    "\n",
    "results = load_results()\n",
    "print(f\"Total results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Flatten Data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_results(results: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Flatten nested JSON results into a DataFrame.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for r in results:\n",
    "        solver_ids = r.get(\"solver_ids\", [])\n",
    "        initial_solutions = r.get(\"initial_solutions\", {})\n",
    "        refined_solutions = r.get(\"refined_solutions\", {})\n",
    "        ground_truth = r.get(\"ground_truth\", \"\")\n",
    "        \n",
    "        # Count how many initial answers match ground truth (simplified check)\n",
    "        initial_correct_count = sum(\n",
    "            1 for sid in solver_ids\n",
    "            if ground_truth.lower() in initial_solutions.get(sid, {}).get(\"final_answer\", \"\").lower()\n",
    "        )\n",
    "        \n",
    "        # Count how many refined answers match ground truth\n",
    "        refined_correct_count = sum(\n",
    "            1 for sid in solver_ids\n",
    "            if ground_truth.lower() in refined_solutions.get(sid, {}).get(\"final_answer\", \"\").lower()\n",
    "        )\n",
    "        \n",
    "        # Get majority vote from refined solutions\n",
    "        refined_answers = [refined_solutions.get(sid, {}).get(\"final_answer\", \"\") for sid in solver_ids]\n",
    "        answer_counts = Counter(refined_answers)\n",
    "        majority_answer = answer_counts.most_common(1)[0][0] if answer_counts else \"\"\n",
    "        majority_would_be_correct = ground_truth.lower() in majority_answer.lower() if majority_answer else False\n",
    "        \n",
    "        row = {\n",
    "            \"problem_id\": r.get(\"problem_id\"),\n",
    "            \"category\": r.get(\"category\", \"Unknown\"),\n",
    "            \"difficulty\": r.get(\"difficulty\", \"Unknown\"),\n",
    "            \"judge_id\": r.get(\"judge_id\"),\n",
    "            \"num_solvers\": len(solver_ids),\n",
    "            \"initial_correct_count\": initial_correct_count,\n",
    "            \"refined_correct_count\": refined_correct_count,\n",
    "            \"final_answer\": r.get(\"verdict\", {}).get(\"final_answer_to_user\", \"\"),\n",
    "            \"best_solver_id\": r.get(\"verdict\", {}).get(\"best_solver_id\", \"\"),\n",
    "            \"is_correct\": r.get(\"evaluation\", {}).get(\"is_correct\", False),\n",
    "            \"majority_answer\": majority_answer,\n",
    "            \"majority_would_be_correct\": majority_would_be_correct,\n",
    "            \"collaboration_improved\": refined_correct_count > initial_correct_count,\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df = flatten_results(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Key Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Accuracy\n",
    "overall_accuracy = df[\"is_correct\"].mean() * 100\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.1f}%\")\n",
    "\n",
    "# Judge Accuracy (how often judge's pick was correct)\n",
    "judge_accuracy = df[\"is_correct\"].mean() * 100\n",
    "print(f\"Judge Selection Accuracy: {judge_accuracy:.1f}%\")\n",
    "\n",
    "# Majority Vote Accuracy\n",
    "majority_accuracy = df[\"majority_would_be_correct\"].mean() * 100\n",
    "print(f\"Majority Vote Accuracy: {majority_accuracy:.1f}%\")\n",
    "\n",
    "# Collaboration Bonus (did refinement help?)\n",
    "collaboration_bonus = df[\"collaboration_improved\"].mean() * 100\n",
    "print(f\"Collaboration Bonus (refinement improved): {collaboration_bonus:.1f}%\")\n",
    "\n",
    "# Accuracy by Category\n",
    "print(\"\\nAccuracy by Category:\")\n",
    "category_accuracy = df.groupby(\"category\")[\"is_correct\"].mean() * 100\n",
    "print(category_accuracy.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Accuracy by Problem Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Calculate accuracy by category\n",
    "category_stats = df.groupby(\"category\").agg(\n",
    "    accuracy=(\"is_correct\", \"mean\"),\n",
    "    count=(\"is_correct\", \"count\")\n",
    ").reset_index()\n",
    "category_stats[\"accuracy\"] = category_stats[\"accuracy\"] * 100\n",
    "\n",
    "# Create bar chart\n",
    "colors = sns.color_palette(\"husl\", len(category_stats))\n",
    "bars = ax.bar(category_stats[\"category\"], category_stats[\"accuracy\"], color=colors, edgecolor=\"black\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, category_stats[\"count\"]):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(\n",
    "        f\"{height:.1f}%\\n(n={count})\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 3),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Problem Category\", fontsize=12)\n",
    "ax.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax.set_title(\"Accuracy by Problem Category\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 110)\n",
    "ax.axhline(y=overall_accuracy, color=\"red\", linestyle=\"--\", label=f\"Overall: {overall_accuracy:.1f}%\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Judge vs Majority Vote Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "methods = [\"Judge Selection\", \"Majority Vote\"]\n",
    "accuracies = [judge_accuracy, majority_accuracy]\n",
    "colors = [\"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "bars = ax.bar(methods, accuracies, color=colors, edgecolor=\"black\", width=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(\n",
    "        f\"{height:.1f}%\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 3),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax.set_title(\"Judge Selection vs Majority Vote Accuracy\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 110)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Initial vs Final Answer Correctness (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion-style matrix:\n",
    "# Rows: Initial had correct answer (at least one solver)\n",
    "# Cols: Final answer correct\n",
    "\n",
    "df[\"initial_had_correct\"] = df[\"initial_correct_count\"] > 0\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_data = pd.crosstab(\n",
    "    df[\"initial_had_correct\"].map({True: \"Initial: Correct\", False: \"Initial: Incorrect\"}),\n",
    "    df[\"is_correct\"].map({True: \"Final: Correct\", False: \"Final: Incorrect\"}),\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_data,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    ax=ax,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    "    annot_kws={\"size\": 16, \"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "ax.set_title(\"Initial Answer vs Final Answer Correctness\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Final Answer\", fontsize=12)\n",
    "ax.set_ylabel(\"Initial Answer (any solver)\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization: Collaboration Bonus (Refinement Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Average correct answers before/after refinement\n",
    "avg_initial = df[\"initial_correct_count\"].mean()\n",
    "avg_refined = df[\"refined_correct_count\"].mean()\n",
    "\n",
    "ax1 = axes[0]\n",
    "stages = [\"Initial Solutions\", \"Refined Solutions\"]\n",
    "avg_correct = [avg_initial, avg_refined]\n",
    "colors = [\"#e74c3c\", \"#27ae60\"]\n",
    "\n",
    "bars = ax1.bar(stages, avg_correct, color=colors, edgecolor=\"black\", width=0.5)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.annotate(\n",
    "        f\"{height:.2f}\",\n",
    "        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "        xytext=(0, 3),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax1.set_ylabel(\"Avg Correct Answers (out of 3 solvers)\", fontsize=12)\n",
    "ax1.set_title(\"Collaboration Impact: Before vs After Refinement\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylim(0, 3.5)\n",
    "\n",
    "# Right: Pie chart of improvement\n",
    "ax2 = axes[1]\n",
    "improved = df[\"collaboration_improved\"].sum()\n",
    "not_improved = len(df) - improved\n",
    "\n",
    "ax2.pie(\n",
    "    [improved, not_improved],\n",
    "    labels=[f\"Improved ({improved})\", f\"No Change/Worse ({not_improved})\"],\n",
    "    colors=[\"#27ae60\", \"#95a5a6\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    explode=(0.05, 0),\n",
    "    textprops={\"fontsize\": 12},\n",
    ")\n",
    "ax2.set_title(\"Problems Where Collaboration Helped\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total Problems\",\n",
    "        \"Overall Accuracy\",\n",
    "        \"Judge Selection Accuracy\",\n",
    "        \"Majority Vote Accuracy\",\n",
    "        \"Collaboration Bonus Rate\",\n",
    "        \"Avg Initial Correct (per problem)\",\n",
    "        \"Avg Refined Correct (per problem)\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        f\"{len(df)}\",\n",
    "        f\"{overall_accuracy:.1f}%\",\n",
    "        f\"{judge_accuracy:.1f}%\",\n",
    "        f\"{majority_accuracy:.1f}%\",\n",
    "        f\"{collaboration_bonus:.1f}%\",\n",
    "        f\"{avg_initial:.2f} / 3\",\n",
    "        f\"{avg_refined:.2f} / 3\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "display(summary.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key columns\n",
    "display_cols = [\n",
    "    \"problem_id\",\n",
    "    \"category\",\n",
    "    \"difficulty\",\n",
    "    \"judge_id\",\n",
    "    \"best_solver_id\",\n",
    "    \"is_correct\",\n",
    "    \"initial_correct_count\",\n",
    "    \"refined_correct_count\",\n",
    "    \"collaboration_improved\",\n",
    "]\n",
    "\n",
    "df[display_cols].style.applymap(\n",
    "    lambda x: \"background-color: #90EE90\" if x is True else (\"background-color: #FFB6C1\" if x is False else \"\"),\n",
    "    subset=[\"is_correct\", \"collaboration_improved\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
