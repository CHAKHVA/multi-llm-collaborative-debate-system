{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-LLM Collaborative Debate System - Demo Playground\n",
    "\n",
    "Interactive notebook for running single debate sessions and observing agent reasoning in real-time.\n",
    "\n",
    "## Features:\n",
    "- Run debates on individual problems\n",
    "- Observe each agent's reasoning process\n",
    "- See peer reviews and refinements\n",
    "- Watch the judge make the final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path(\".\").resolve().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from src.agents import PERSONAS\n",
    "from src.orchestrator import (\n",
    "    get_role_preference,\n",
    "    generate_solution,\n",
    "    generate_critique,\n",
    "    refine_solution,\n",
    "    judge_verdict,\n",
    "    grade_answer,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 problems\n",
      "\n",
      "Available problems:\n",
      "  [1] Logic/Hard: Three gods A, B, and C are called, in no particular order, T...\n",
      "  [2] Math/Medium: Find the last two digits of 7^2024....\n",
      "  [3] Physics/Medium: A ladder of length L and mass m leans against a frictionless...\n",
      "  [4] Game Theory/Hard: In a sealed-bid second-price auction (Vickrey auction) with ...\n",
      "  [5] Logic/Hard: Cheryl gives Albert and Bernard ten possible dates for her b...\n",
      "  [6] Math/Hard: A standard 12-hour clock has an hour hand and a minute hand....\n",
      "  [7] Probability/Medium: You are on a game show with 3 doors. Behind one is a car; be...\n",
      "  [8] Math/Medium: If x^x^x... (infinite power tower) = 2, what is x?...\n",
      "  [9] Physics/Medium: A car accelerates from rest at a constant rate 'a' for time ...\n",
      "  [10] Logic/Hard: 100 prisoners are lined up. Each wears a red or blue hat. Th...\n"
     ]
    }
   ],
   "source": [
    "PROBLEMS_PATH = Path(\"../data/problems.json\")\n",
    "\n",
    "with open(PROBLEMS_PATH, \"r\") as f:\n",
    "    problems = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(problems)} problems\")\n",
    "print(\"\\nAvailable problems:\")\n",
    "for p in problems[:10]:  # Show first 10\n",
    "    print(f\"  [{p['id']}] {p['category']}/{p['difficulty']}: {p['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select a Problem\n",
    "\n",
    "Choose a problem ID or define a custom problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Problem 1\n",
      "Category: Logic\n",
      "Difficulty: Hard\n",
      "\n",
      "Question:\n",
      "Three gods A, B, and C are called, in no particular order, True, False, and Random. True always speaks truly, False always speaks falsely, but whether Random speaks truly or falsely is a completely random matter. You must determine the identities of A, B, and C by asking three yes-no questions; each question must be put to exactly one god. The gods understand English, but will answer all questions in their own language, in which the words for yes and no are 'da' and 'ja', in some order. You do not know which word means which. What is the first question you should ask to ensure you can solve the puzzle?\n",
      "\n",
      "Ground Truth: Ask god B: 'If I asked you 'Is A Random?', would you say 'ja'?' (If B answers 'ja', then C is not Random. If B answers 'da', then A is not Random. This isolates a non-random god to ask the next questions to.)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Select by ID\n",
    "PROBLEM_ID = 1  # Change this to test different problems\n",
    "\n",
    "problem = next((p for p in problems if p[\"id\"] == PROBLEM_ID), None)\n",
    "\n",
    "# Option 2: Define a custom problem (uncomment to use)\n",
    "# problem = {\n",
    "#     \"id\": 999,\n",
    "#     \"category\": \"Custom\",\n",
    "#     \"difficulty\": \"Medium\",\n",
    "#     \"question\": \"What is 2 + 2?\",\n",
    "#     \"ground_truth\": \"4\",\n",
    "# }\n",
    "\n",
    "if problem:\n",
    "    print(f\"Selected Problem {problem['id']}\")\n",
    "    print(f\"Category: {problem['category']}\")\n",
    "    print(f\"Difficulty: {problem['difficulty']}\")\n",
    "    print(f\"\\nQuestion:\\n{problem['question']}\")\n",
    "    print(f\"\\nGround Truth: {problem['ground_truth']}\")\n",
    "else:\n",
    "    print(f\"Problem ID {PROBLEM_ID} not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions for Pretty Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(title: str) -> None:\n",
    "    \"\"\"Print a formatted header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def print_subheader(title: str) -> None:\n",
    "    \"\"\"Print a formatted subheader.\"\"\"\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "\n",
    "\n",
    "def print_agent_info(agent_id: str) -> None:\n",
    "    \"\"\"Print agent persona information.\"\"\"\n",
    "    print(f\"\\nAgent {agent_id} Persona: {PERSONAS[agent_id][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 0: Role Assignment\n",
    "\n",
    "Each agent decides if they prefer to be a Solver or Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " STAGE 0: ROLE ASSIGNMENT\n",
      "================================================================================\n",
      "\n",
      "--- Agent A ---\n",
      "\n",
      "Agent A Persona: You are a rigid logician and scientist. You prioritize formal proofs, step-by-step derivation, and e...\n",
      "\n",
      "Preferred Role: Solver\n",
      "Confidence: 0.90\n",
      "Reasoning: Given the complexity and the logical nature of the problem, it requires a structured approach to formulating questions and deducing the identities of the gods based on their responses. As a Solver, I ...\n",
      "\n",
      "--- Agent B ---\n",
      "\n",
      "Agent B Persona: You are a lateral thinker. You look for alternative interpretations, trick wording, or creative solu...\n",
      "\n",
      "Preferred Role: Solver\n",
      "Confidence: 0.85\n",
      "Reasoning: I believe I would be better suited as a Solver for this problem because it requires creative reasoning and lateral thinking to derive an effective first question. This involves understanding the nuanc...\n",
      "\n",
      "--- Agent C ---\n",
      "\n",
      "Agent C Persona: You are a pragmatic engineer. You focus on probability, heuristics, and real-world constraints. You ...\n",
      "\n",
      "Preferred Role: Solver\n",
      "Confidence: 0.85\n",
      "Reasoning: This problem is a classic logic puzzle that requires deep reasoning and creativity to solve effectively. As a Solver, I can generate innovative approaches and navigate the complexities involved in for...\n",
      "\n",
      "--- Agent D ---\n",
      "\n",
      "Agent D Persona: You are a balanced synthesizer. You strive for clarity and consensus. You try to bridge the gap betw...\n",
      "\n",
      "Preferred Role: Solver\n",
      "Confidence: 0.85\n",
      "Reasoning: As a Solver, I can engage directly with the puzzle, applying logical reasoning to deduce the identities of the gods with the limited number of questions allowed. The complexity of the problem requires...\n"
     ]
    }
   ],
   "source": [
    "print_header(\"STAGE 0: ROLE ASSIGNMENT\")\n",
    "\n",
    "question = problem[\"question\"]\n",
    "agent_ids = list(PERSONAS.keys())\n",
    "preferences = []\n",
    "\n",
    "for agent_id in agent_ids:\n",
    "    print_subheader(f\"Agent {agent_id}\")\n",
    "    print_agent_info(agent_id)\n",
    "    \n",
    "    pref = get_role_preference(client, agent_id, question)\n",
    "    preferences.append(pref)\n",
    "    \n",
    "    print(f\"\\nPreferred Role: {pref.role_priority}\")\n",
    "    print(f\"Confidence: {pref.confidence:.2f}\")\n",
    "    print(f\"Reasoning: {pref.reasoning[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROLE ASSIGNMENT RESULT ---\n",
      "Judge: Agent D\n",
      "Solvers: Agents A, B, C\n"
     ]
    }
   ],
   "source": [
    "# Assign roles based on preferences\n",
    "import random\n",
    "\n",
    "judge_candidates = [p for p in preferences if p.role_priority == \"Judge\"]\n",
    "\n",
    "if judge_candidates:\n",
    "    weights = [p.confidence for p in judge_candidates]\n",
    "    judge_pref = random.choices(judge_candidates, weights=weights, k=1)[0]\n",
    "    judge_id = judge_pref.agent_id\n",
    "else:\n",
    "    judge_pref = random.choice(preferences)\n",
    "    judge_id = judge_pref.agent_id\n",
    "\n",
    "solver_ids = [p.agent_id for p in preferences if p.agent_id != judge_id]\n",
    "\n",
    "print_subheader(\"ROLE ASSIGNMENT RESULT\")\n",
    "print(f\"Judge: Agent {judge_id}\")\n",
    "print(f\"Solvers: Agents {', '.join(solver_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 1: Independent Solutions\n",
    "\n",
    "Each solver generates their solution independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " STAGE 1: INDEPENDENT SOLUTIONS\n",
      "================================================================================\n",
      "\n",
      "--- Solver A ---\n",
      "\n",
      "Agent A Persona: You are a rigid logician and scientist. You prioritize formal proofs, step-by-step derivation, and e...\n",
      "\n",
      "Solution:\n",
      "To solve the problem of identifying the three gods (True, False, and Random) with the constraints that they answer only in ‘da’ and ‘ja’ (which we don't know the meanings of), we need to construct our first question carefully. \n",
      "\n",
      "1. **Identifying the Types of Gods**: We note that True will always tell the truth, False will always lie, and Random can give any answer (honestly or dishonestly). \n",
      "\n",
      "2. **Understanding the Responses**: The two words 'da' and 'ja' can mean either 'yes' or 'no'. We cannot...\n",
      "\n",
      "FINAL ANSWER: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n",
      "\n",
      "--- Solver B ---\n",
      "\n",
      "Agent B Persona: You are a lateral thinker. You look for alternative interpretations, trick wording, or creative solu...\n",
      "\n",
      "Solution:\n",
      "To tackle this problem, we must devise a first question that enables us to deduce key information about at least one of the gods, despite the ambiguity of their truthfulness and the meaning of \"da\" and \"ja\".\n",
      "\n",
      "### Step 1: Understanding the Characters\n",
      "- **True**: Always tells the truth.\n",
      "- **False**: Always lies.\n",
      "- **Random**: Can tell the truth or lie at any time, so we cannot rely on their answers.\n",
      "\n",
      "### Step 2: The Language Barrier\n",
      "- The words for \"yes\" and \"no\" are unknown to us but one is \"da\" ...\n",
      "\n",
      "FINAL ANSWER: \"If I asked you 'Is A True?', would you say 'da'?\"\n",
      "\n",
      "--- Solver C ---\n",
      "\n",
      "Agent C Persona: You are a pragmatic engineer. You focus on probability, heuristics, and real-world constraints. You ...\n",
      "\n",
      "Solution:\n",
      "To solve the problem of identifying the three gods: True, False, and Random, we need to devise a question that will yield useful information regardless of the identity of the god we ask. The main issues we must deal with are not only the identities of the gods but also the uncertainty of their responses ('da' and 'ja' being responses for yes or no in an unknown order). \n",
      "\n",
      "1. **Understanding responses:** \n",
      "   - True always tells the truth. \n",
      "   - False always lies. \n",
      "   - Random can say either truth ...\n",
      "\n",
      "FINAL ANSWER: Ask: \"If I asked you 'Is God B Random?', would you say 'da'?\"\n"
     ]
    }
   ],
   "source": [
    "print_header(\"STAGE 1: INDEPENDENT SOLUTIONS\")\n",
    "\n",
    "initial_solutions = {}\n",
    "\n",
    "for solver_id in solver_ids:\n",
    "    print_subheader(f\"Solver {solver_id}\")\n",
    "    print_agent_info(solver_id)\n",
    "    \n",
    "    solution = generate_solution(client, solver_id, question)\n",
    "    initial_solutions[solver_id] = solution\n",
    "    \n",
    "    print(\"\\nSolution:\")\n",
    "    print(f\"{solution.solution_text[:500]}...\" if len(solution.solution_text) > 500 else solution.solution_text)\n",
    "    print(f\"\\nFINAL ANSWER: {solution.final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 2: Peer Review (Round Robin)\n",
    "\n",
    "Each solver critiques the other two solvers' work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " STAGE 2: PEER REVIEW\n",
      "================================================================================\n",
      "\n",
      "--- Reviewer A -> Target B ---\n",
      "\n",
      "Score: 5/10\n",
      "Strengths: The solver correctly identified the challenges posed by the nature of the gods and the ambiguity of their language., The proposed question is structured to gather information about both the identity of a god and the interpretation of their responses.\n",
      "Weaknesses: The solution lacks clarity in the evaluation of responses, particularly in the case when Random is asked., The impact of asking Random is downplayed – its unpredictability should have been more thoroughly explored, as it complicates the derivation of reliable knowledge.\n",
      "Errors Found: 2\n",
      "  - [critical] Step 4: The analysis does not adequately consider that responses from Random may confuse the interpretation \n",
      "  - [critical] Step 5: The conclusion implies that any answer could lead towards identifying True or False; however, the ro\n",
      "\n",
      "--- Reviewer A -> Target C ---\n",
      "\n",
      "Score: 6/10\n",
      "Strengths: The solver correctly identified the need for a self-referential question to navigate the complexities of the gods' identities and the meanings of 'da' and 'ja'., The formulation of the question is well-structured, guiding towards clarity about both the answers and the identities of A, B, and C.\n",
      "Weaknesses: The solver did not fully analyze the implications of asking Random, which complicates the derived information from the response., The reasoning does not adequately address all scenarios that may arise based on the identities of the gods, particularly how to definitively determine the meaning of 'da' and 'ja' without ambiguity.\n",
      "Errors Found: 1\n",
      "  - [critical] Step 4: The analysis of Random's response is incomplete; the question only examines the outcomes for True an\n",
      "\n",
      "--- Reviewer B -> Target A ---\n",
      "\n",
      "Score: 5/10\n",
      "Strengths: The solution correctly identifies the goal of determining the identities of the gods with a careful questioning strategy., The proposed question leverages the structure of the problem effectively by using a double layer of inquiry (asking about another god)., The solver considers the implications of answers based on the identities of the gods, showing an understanding of logical deduction.\n",
      "Weaknesses: The proposed question does not account properly for the randomness of Random's answer, which could lead to confusion about how to proceed based on that uncertainty., The framework for responding to Random's answer is underdeveloped - it does not detail how to handle Random's unpredictability effectively in subsequent questions., The explanation implies that the answers can lead directly to determining the identities of the gods, but this is not necessarily guaranteed without a precise follow-up questioning strategy.\n",
      "Errors Found: 1\n",
      "  - [critical] Step 6: The logic in Step 6 implies that we can directly conclude B's identity from A's response, which is m\n",
      "\n",
      "--- Reviewer B -> Target C ---\n",
      "\n",
      "Score: 5/10\n",
      "Strengths: The solver correctly recognizes the importance of framing the question to account for the uncertain meanings of 'da' and 'ja'., They approach the problem methodically, considering the responses that different gods would give based on their truth-telling nature., The solver identifies a self-referential strategy, which is a clever method commonly used in logical puzzles.\n",
      "Weaknesses: The solution fails to account for the potential randomness of the Random god properly; asking Random yields no useful information and may mislead after the first question., The analysis does not provide a clear breakdown of outcomes for Random which could lead to ambiguity in interpreting the resulting answers.\n",
      "Errors Found: 1\n",
      "  - [critical] Step 4: The outcomes of the question asked to Random are vague and not fully considered, potentially leading\n",
      "\n",
      "--- Reviewer C -> Target A ---\n",
      "\n",
      "Score: 7/10\n",
      "Strengths: The solution clearly explains the reasoning behind the formulation of the question., It correctly identifies the need to account for the potentially misleading responses from the gods., The approach effectively considers the roles of True and False in response to the question.\n",
      "Weaknesses: The explanation could benefit from more clarity regarding the distinction of Random's influence on the question's outcome., The solution does not sufficiently address what should be done after determining the outcome of the first question, leaving the reader without a full strategy for all three questions.\n",
      "Errors Found: 1\n",
      "  - [minor] Step 6: The analysis does not adequately explore how to address the possibility of getting an answer from Ra\n",
      "\n",
      "--- Reviewer C -> Target B ---\n",
      "\n",
      "Score: 6/10\n",
      "Strengths: The solver correctly identifies the need to consider both the nature of each god and the ambiguity in the language., The construction of the question is logical and demonstrates an understanding of how to create a self-referential question that incorporates both truthfulness and potential randomness., The analysis of responses provides a reasonable framework for interpreting the answers we may receive from the gods.\n",
      "Weaknesses: The reasoning around how to interpret the answers to the question posed is somewhat convoluted and lacks clarity., While the solver aims to cover all bases, the approach could potentially lead to confusion when deciphering the meaning of the responses from the gods., The step analyzing the response of Random could suggest asking Random instead of True or False, which may lead nowhere in terms of useful information.\n",
      "Errors Found: 2\n",
      "  - [minor] Step 4: The explanation of how to analyze the responses from True and False is unclear and could mislead the\n",
      "  - [critical] Step 3: The chosen question does not sufficiently distinguish between True, False, and Random. The solver fa\n"
     ]
    }
   ],
   "source": [
    "print_header(\"STAGE 2: PEER REVIEW\")\n",
    "\n",
    "all_reviews = {sid: [] for sid in solver_ids}\n",
    "\n",
    "for reviewer_id in solver_ids:\n",
    "    for target_id in solver_ids:\n",
    "        if reviewer_id != target_id:\n",
    "            print_subheader(f\"Reviewer {reviewer_id} -> Target {target_id}\")\n",
    "            \n",
    "            review = generate_critique(\n",
    "                client,\n",
    "                reviewer_id,\n",
    "                target_id,\n",
    "                question,\n",
    "                initial_solutions[target_id],\n",
    "            )\n",
    "            all_reviews[target_id].append(review)\n",
    "            \n",
    "            print(f\"\\nScore: {review.score}/10\")\n",
    "            print(f\"Strengths: {', '.join(review.strengths[:3])}\")\n",
    "            print(f\"Weaknesses: {', '.join(review.weaknesses[:3])}\")\n",
    "            if review.errors:\n",
    "                print(f\"Errors Found: {len(review.errors)}\")\n",
    "                for err in review.errors[:2]:\n",
    "                    print(f\"  - [{err.severity}] {err.location}: {err.description[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 3: Refinement\n",
    "\n",
    "Solvers improve their solutions based on peer feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " STAGE 3: REFINEMENT\n",
      "================================================================================\n",
      "\n",
      "--- Solver A Refining ---\n",
      "\n",
      "Changes Made: Clarified the analysis of Random's responses and specified how to handle ambiguous responses. Enhanced the explanation of the consequences of each answer from the first question and provided a follow-up question strategy.\n",
      "\n",
      "Original Answer: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n",
      "Refined Answer:  Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n",
      "\n",
      "--- Solver B Refining ---\n",
      "\n",
      "Changes Made: Clarified the analysis of responses to include the unpredictability of Random more explicitly. Explained the implications of each possible answer more clearly and reinforced that identifying Random complicates the deduction process. Revised the question phrasing to ensure it directly targets the ide...\n",
      "\n",
      "Original Answer: \"If I asked you 'Is A True?', would you say 'da'?\"\n",
      "Refined Answer:  If I asked you whether 'You are Random', would you say 'da'?\n",
      ">>> ANSWER CHANGED!\n",
      "\n",
      "--- Solver C Refining ---\n",
      "\n",
      "Changes Made: Expanded the analysis of each possible scenario, especially the case when asking Random, to clarify how to interpret responses and determine the identities of the gods. Provided additional steps for follow-up questions to further dispel ambiguity.\n",
      "\n",
      "Original Answer: Ask: \"If I asked you 'Is God B Random?', would you say 'da'?\"\n",
      "Refined Answer:  Ask: \"If I asked you 'Is God B Random?', would you say 'da'?\"\n"
     ]
    }
   ],
   "source": [
    "print_header(\"STAGE 3: REFINEMENT\")\n",
    "\n",
    "refined_solutions = {}\n",
    "\n",
    "for solver_id in solver_ids:\n",
    "    print_subheader(f\"Solver {solver_id} Refining\")\n",
    "    \n",
    "    refined = refine_solution(\n",
    "        client,\n",
    "        solver_id,\n",
    "        question,\n",
    "        initial_solutions[solver_id],\n",
    "        all_reviews[solver_id],\n",
    "    )\n",
    "    refined_solutions[solver_id] = refined\n",
    "    \n",
    "    print(f\"\\nChanges Made: {refined.changes_made[:300]}...\" if len(refined.changes_made) > 300 else f\"\\nChanges Made: {refined.changes_made}\")\n",
    "    print(f\"\\nOriginal Answer: {initial_solutions[solver_id].final_answer}\")\n",
    "    print(f\"Refined Answer:  {refined.final_answer}\")\n",
    "    \n",
    "    # Check if answer changed\n",
    "    if initial_solutions[solver_id].final_answer != refined.final_answer:\n",
    "        print(\">>> ANSWER CHANGED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stage 4: Judge Verdict\n",
    "\n",
    "The judge evaluates all solutions and selects the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " STAGE 4: JUDGE VERDICT\n",
      "================================================================================\n",
      "\n",
      "Judge: Agent D\n",
      "\n",
      "Agent D Persona: You are a balanced synthesizer. You strive for clarity and consensus. You try to bridge the gap betw...\n",
      "\n",
      "WINNER: Solver A\n",
      "\n",
      "Rationale:\n",
      "Solver A's final answer is the strongest due to its clear, logical construction of the first question that directly addresses the challenge of identifying the identities of the gods under the constraints given. Despite receiving some critiques about how Random's response is addressed, A's explanation showcases a consistent and solid understanding of the deductive reasoning necessary to navigate the complexities of the problem. Additionally, their response shows an awareness of the need for follow-up questions based on the answers given, allowing for a more robust strategy in tackling the puzzle. The overall clarity and logical progression of thoughts in their refined solution contribute to its effectiveness in answering the problem at hand.\n",
      "\n",
      "FINAL ANSWER TO USER: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n"
     ]
    }
   ],
   "source": [
    "print_header(\"STAGE 4: JUDGE VERDICT\")\n",
    "\n",
    "print(f\"\\nJudge: Agent {judge_id}\")\n",
    "print_agent_info(judge_id)\n",
    "\n",
    "verdict = judge_verdict(\n",
    "    client,\n",
    "    judge_id,\n",
    "    question,\n",
    "    solver_ids,\n",
    "    initial_solutions,\n",
    "    all_reviews,\n",
    "    refined_solutions,\n",
    ")\n",
    "\n",
    "print(f\"\\nWINNER: Solver {verdict.best_solver_id}\")\n",
    "print(\"\\nRationale:\")\n",
    "print(verdict.rationale)\n",
    "print(f\"\\nFINAL ANSWER TO USER: {verdict.final_answer_to_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "Compare the final answer against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Ground Truth: Ask god B: 'If I asked you 'Is A Random?', would you say 'ja'?' (If B answers 'ja', then C is not Random. If B answers 'da', then A is not Random. This isolates a non-random god to ask the next questions to.)\n",
      "System Answer: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n",
      "\n",
      "Correct: NO\n",
      "\n",
      "Evaluation Reasoning:\n",
      "The proposed question to god A does not effectively isolate the identity of the gods in the same manner as the ground truth. While asking about god B is a step, the structure of the question does not allow us to deduce the status of god B in relation to true, false, or random as effectively as the ground truth does. The ground truth question is designed to specifically identify one of the gods as either True or False by asking about god A's identity in relation to Random. The answer provided does not accomplish this same level of clarity or isolation.\n"
     ]
    }
   ],
   "source": [
    "print_header(\"EVALUATION\")\n",
    "\n",
    "ground_truth = problem[\"ground_truth\"]\n",
    "\n",
    "evaluation = grade_answer(\n",
    "    client,\n",
    "    question,\n",
    "    ground_truth,\n",
    "    verdict.final_answer_to_user,\n",
    ")\n",
    "\n",
    "print(f\"\\nGround Truth: {ground_truth}\")\n",
    "print(f\"System Answer: {verdict.final_answer_to_user}\")\n",
    "print(f\"\\nCorrect: {'YES' if evaluation.is_correct else 'NO'}\")\n",
    "print(\"\\nEvaluation Reasoning:\")\n",
    "print(evaluation.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " DEBATE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Problem ID: 1\n",
      "Category: Logic\n",
      "Difficulty: Hard\n",
      "\n",
      "Roles:\n",
      "  - Judge: Agent D\n",
      "  - Solvers: A, B, C\n",
      "\n",
      "Initial Answers:\n",
      "\n",
      "  - Solver A: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n",
      "  - Solver B: \"If I asked you 'Is A True?', would you say 'da'?\"\n",
      "  - Solver C: Ask: \"If I asked you 'Is God B Random?', would you say 'da'?\"\n",
      "\n",
      "Refined Answers:\n",
      "\n",
      "  - Solver A: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\" \n",
      "  - Solver B: If I asked you whether 'You are Random', would you say 'da'? (changed)\n",
      "  - Solver C: Ask: \"If I asked you 'Is God B Random?', would you say 'da'?\" \n",
      "\n",
      "Judge Selected: Solver A\n",
      "Final Answer: Ask god A: \"If I asked you whether god B is Random, would you say 'da'?\"\n",
      "\n",
      "Ground Truth: Ask god B: 'If I asked you 'Is A Random?', would you say 'ja'?' (If B answers 'ja', then C is not Random. If B answers 'da', then A is not Random. This isolates a non-random god to ask the next questions to.)\n",
      "Result: INCORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_header(\"DEBATE SUMMARY\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Problem ID: {problem['id']}\n",
    "Category: {problem['category']}\n",
    "Difficulty: {problem['difficulty']}\n",
    "\n",
    "Roles:\n",
    "  - Judge: Agent {judge_id}\n",
    "  - Solvers: {', '.join(solver_ids)}\n",
    "\n",
    "Initial Answers:\n",
    "\"\"\")\n",
    "for sid in solver_ids:\n",
    "    print(f\"  - Solver {sid}: {initial_solutions[sid].final_answer}\")\n",
    "\n",
    "print(\"\"\"\n",
    "Refined Answers:\n",
    "\"\"\")\n",
    "for sid in solver_ids:\n",
    "    changed = \"(changed)\" if initial_solutions[sid].final_answer != refined_solutions[sid].final_answer else \"\"\n",
    "    print(f\"  - Solver {sid}: {refined_solutions[sid].final_answer} {changed}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Judge Selected: Solver {verdict.best_solver_id}\n",
    "Final Answer: {verdict.final_answer_to_user}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "Result: {'CORRECT' if evaluation.is_correct else 'INCORRECT'}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Run: Full Debate Pipeline\n",
    "\n",
    "Use this cell to run a complete debate in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running debate for Problem 7: You are on a game show with 3 doors. Behind one is a car; behind the others, goa...\n",
      "  Roles: Judge=D, Solvers=['A', 'B', 'C']\n",
      "  Initial answers: ['2/3', '\\\\frac{2}{3}', '2/3']\n",
      "  Peer reviews completed\n",
      "  Refined answers: ['2/3', 'The probability of winning if ', '2/3']\n",
      "  Judge selected: Solver A\n",
      "  Final answer: The probability of winning the car if you switch to Door 2 is 2/3.\n",
      "  Ground truth: 2/3\n",
      "  Result: CORRECT\n"
     ]
    }
   ],
   "source": [
    "def run_full_debate(problem_id: int) -> dict:\n",
    "    \"\"\"Run a complete debate for a given problem ID.\"\"\"\n",
    "    problem = next((p for p in problems if p[\"id\"] == problem_id), None)\n",
    "    if not problem:\n",
    "        raise ValueError(f\"Problem ID {problem_id} not found\")\n",
    "    \n",
    "    question = problem[\"question\"]\n",
    "    print(f\"Running debate for Problem {problem_id}: {question[:80]}...\")\n",
    "    \n",
    "    # Stage 0: Role assignment\n",
    "    preferences = [get_role_preference(client, aid, question) for aid in PERSONAS.keys()]\n",
    "    judge_candidates = [p for p in preferences if p.role_priority == \"Judge\"]\n",
    "    if judge_candidates:\n",
    "        judge_id = random.choices(judge_candidates, weights=[p.confidence for p in judge_candidates], k=1)[0].agent_id\n",
    "    else:\n",
    "        judge_id = random.choice(preferences).agent_id\n",
    "    solver_ids = [p.agent_id for p in preferences if p.agent_id != judge_id]\n",
    "    print(f\"  Roles: Judge={judge_id}, Solvers={solver_ids}\")\n",
    "    \n",
    "    # Stage 1: Solutions\n",
    "    initial_solutions = {sid: generate_solution(client, sid, question) for sid in solver_ids}\n",
    "    print(f\"  Initial answers: {[initial_solutions[s].final_answer[:30] for s in solver_ids]}\")\n",
    "    \n",
    "    # Stage 2: Reviews\n",
    "    all_reviews = {sid: [] for sid in solver_ids}\n",
    "    for reviewer in solver_ids:\n",
    "        for target in solver_ids:\n",
    "            if reviewer != target:\n",
    "                all_reviews[target].append(\n",
    "                    generate_critique(client, reviewer, target, question, initial_solutions[target])\n",
    "                )\n",
    "    print(\"  Peer reviews completed\")\n",
    "    \n",
    "    # Stage 3: Refinement\n",
    "    refined_solutions = {\n",
    "        sid: refine_solution(client, sid, question, initial_solutions[sid], all_reviews[sid])\n",
    "        for sid in solver_ids\n",
    "    }\n",
    "    print(f\"  Refined answers: {[refined_solutions[s].final_answer[:30] for s in solver_ids]}\")\n",
    "    \n",
    "    # Stage 4: Verdict\n",
    "    verdict = judge_verdict(client, judge_id, question, solver_ids, initial_solutions, all_reviews, refined_solutions)\n",
    "    print(f\"  Judge selected: Solver {verdict.best_solver_id}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation = grade_answer(client, question, problem[\"ground_truth\"], verdict.final_answer_to_user)\n",
    "    print(f\"  Final answer: {verdict.final_answer_to_user}\")\n",
    "    print(f\"  Ground truth: {problem['ground_truth']}\")\n",
    "    print(f\"  Result: {'CORRECT' if evaluation.is_correct else 'INCORRECT'}\")\n",
    "    \n",
    "    return {\n",
    "        \"problem\": problem,\n",
    "        \"verdict\": verdict,\n",
    "        \"evaluation\": evaluation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Run on problem 7 (Monty Hall)\n",
    "result = run_full_debate(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
