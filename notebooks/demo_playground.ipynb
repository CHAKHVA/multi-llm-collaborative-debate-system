{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-LLM Collaborative Debate System - Demo Playground\n",
    "\n",
    "Interactive notebook for running single debate sessions and observing agent reasoning in real-time.\n",
    "\n",
    "## Features:\n",
    "- Run debates on individual problems\n",
    "- Observe each agent's reasoning process\n",
    "- See peer reviews and refinements\n",
    "- Watch the judge make the final decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, str(Path(\".\").resolve().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from src.agents import PERSONAS\n",
    "from src.orchestrator import (\n",
    "    get_role_preference,\n",
    "    generate_solution,\n",
    "    generate_critique,\n",
    "    refine_solution,\n",
    "    judge_verdict,\n",
    "    grade_answer,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Problem Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBLEMS_PATH = Path(\"../data/problems.json\")\n",
    "\n",
    "with open(PROBLEMS_PATH, \"r\") as f:\n",
    "    problems = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(problems)} problems\")\n",
    "print(\"\\nAvailable problems:\")\n",
    "for p in problems[:10]:  # Show first 10\n",
    "    print(f\"  [{p['id']}] {p['category']}/{p['difficulty']}: {p['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select a Problem\n",
    "\n",
    "Choose a problem ID or define a custom problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Select by ID\n",
    "PROBLEM_ID = 1  # Change this to test different problems\n",
    "\n",
    "problem = next((p for p in problems if p[\"id\"] == PROBLEM_ID), None)\n",
    "\n",
    "# Option 2: Define a custom problem (uncomment to use)\n",
    "# problem = {\n",
    "#     \"id\": 999,\n",
    "#     \"category\": \"Custom\",\n",
    "#     \"difficulty\": \"Medium\",\n",
    "#     \"question\": \"What is 2 + 2?\",\n",
    "#     \"ground_truth\": \"4\",\n",
    "# }\n",
    "\n",
    "if problem:\n",
    "    print(f\"Selected Problem {problem['id']}\")\n",
    "    print(f\"Category: {problem['category']}\")\n",
    "    print(f\"Difficulty: {problem['difficulty']}\")\n",
    "    print(f\"\\nQuestion:\\n{problem['question']}\")\n",
    "    print(f\"\\nGround Truth: {problem['ground_truth']}\")\n",
    "else:\n",
    "    print(f\"Problem ID {PROBLEM_ID} not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions for Pretty Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(title: str) -> None:\n",
    "    \"\"\"Print a formatted header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\" {title}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def print_subheader(title: str) -> None:\n",
    "    \"\"\"Print a formatted subheader.\"\"\"\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "\n",
    "\n",
    "def print_agent_info(agent_id: str) -> None:\n",
    "    \"\"\"Print agent persona information.\"\"\"\n",
    "    print(f\"\\nAgent {agent_id} Persona: {PERSONAS[agent_id][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 0: Role Assignment\n",
    "\n",
    "Each agent decides if they prefer to be a Solver or Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"STAGE 0: ROLE ASSIGNMENT\")\n",
    "\n",
    "question = problem[\"question\"]\n",
    "agent_ids = list(PERSONAS.keys())\n",
    "preferences = []\n",
    "\n",
    "for agent_id in agent_ids:\n",
    "    print_subheader(f\"Agent {agent_id}\")\n",
    "    print_agent_info(agent_id)\n",
    "    \n",
    "    pref = get_role_preference(client, agent_id, question)\n",
    "    preferences.append(pref)\n",
    "    \n",
    "    print(f\"\\nPreferred Role: {pref.role_priority}\")\n",
    "    print(f\"Confidence: {pref.confidence:.2f}\")\n",
    "    print(f\"Reasoning: {pref.reasoning[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign roles based on preferences\n",
    "import random\n",
    "\n",
    "judge_candidates = [p for p in preferences if p.role_priority == \"Judge\"]\n",
    "\n",
    "if judge_candidates:\n",
    "    weights = [p.confidence for p in judge_candidates]\n",
    "    judge_pref = random.choices(judge_candidates, weights=weights, k=1)[0]\n",
    "    judge_id = judge_pref.agent_id\n",
    "else:\n",
    "    judge_pref = random.choice(preferences)\n",
    "    judge_id = judge_pref.agent_id\n",
    "\n",
    "solver_ids = [p.agent_id for p in preferences if p.agent_id != judge_id]\n",
    "\n",
    "print_subheader(\"ROLE ASSIGNMENT RESULT\")\n",
    "print(f\"Judge: Agent {judge_id}\")\n",
    "print(f\"Solvers: Agents {', '.join(solver_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 1: Independent Solutions\n",
    "\n",
    "Each solver generates their solution independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"STAGE 1: INDEPENDENT SOLUTIONS\")\n",
    "\n",
    "initial_solutions = {}\n",
    "\n",
    "for solver_id in solver_ids:\n",
    "    print_subheader(f\"Solver {solver_id}\")\n",
    "    print_agent_info(solver_id)\n",
    "    \n",
    "    solution = generate_solution(client, solver_id, question)\n",
    "    initial_solutions[solver_id] = solution\n",
    "    \n",
    "    print(f\"\\nSolution:\")\n",
    "    print(f\"{solution.solution_text[:500]}...\" if len(solution.solution_text) > 500 else solution.solution_text)\n",
    "    print(f\"\\nFINAL ANSWER: {solution.final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 2: Peer Review (Round Robin)\n",
    "\n",
    "Each solver critiques the other two solvers' work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"STAGE 2: PEER REVIEW\")\n",
    "\n",
    "all_reviews = {sid: [] for sid in solver_ids}\n",
    "\n",
    "for reviewer_id in solver_ids:\n",
    "    for target_id in solver_ids:\n",
    "        if reviewer_id != target_id:\n",
    "            print_subheader(f\"Reviewer {reviewer_id} -> Target {target_id}\")\n",
    "            \n",
    "            review = generate_critique(\n",
    "                client,\n",
    "                reviewer_id,\n",
    "                target_id,\n",
    "                question,\n",
    "                initial_solutions[target_id],\n",
    "            )\n",
    "            all_reviews[target_id].append(review)\n",
    "            \n",
    "            print(f\"\\nScore: {review.score}/10\")\n",
    "            print(f\"Strengths: {', '.join(review.strengths[:3])}\")\n",
    "            print(f\"Weaknesses: {', '.join(review.weaknesses[:3])}\")\n",
    "            if review.errors:\n",
    "                print(f\"Errors Found: {len(review.errors)}\")\n",
    "                for err in review.errors[:2]:\n",
    "                    print(f\"  - [{err.severity}] {err.location}: {err.description[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 3: Refinement\n",
    "\n",
    "Solvers improve their solutions based on peer feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"STAGE 3: REFINEMENT\")\n",
    "\n",
    "refined_solutions = {}\n",
    "\n",
    "for solver_id in solver_ids:\n",
    "    print_subheader(f\"Solver {solver_id} Refining\")\n",
    "    \n",
    "    refined = refine_solution(\n",
    "        client,\n",
    "        solver_id,\n",
    "        question,\n",
    "        initial_solutions[solver_id],\n",
    "        all_reviews[solver_id],\n",
    "    )\n",
    "    refined_solutions[solver_id] = refined\n",
    "    \n",
    "    print(f\"\\nChanges Made: {refined.changes_made[:300]}...\" if len(refined.changes_made) > 300 else f\"\\nChanges Made: {refined.changes_made}\")\n",
    "    print(f\"\\nOriginal Answer: {initial_solutions[solver_id].final_answer}\")\n",
    "    print(f\"Refined Answer:  {refined.final_answer}\")\n",
    "    \n",
    "    # Check if answer changed\n",
    "    if initial_solutions[solver_id].final_answer != refined.final_answer:\n",
    "        print(\">>> ANSWER CHANGED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stage 4: Judge Verdict\n",
    "\n",
    "The judge evaluates all solutions and selects the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"STAGE 4: JUDGE VERDICT\")\n",
    "\n",
    "print(f\"\\nJudge: Agent {judge_id}\")\n",
    "print_agent_info(judge_id)\n",
    "\n",
    "verdict = judge_verdict(\n",
    "    client,\n",
    "    judge_id,\n",
    "    question,\n",
    "    solver_ids,\n",
    "    initial_solutions,\n",
    "    all_reviews,\n",
    "    refined_solutions,\n",
    ")\n",
    "\n",
    "print(f\"\\nWINNER: Solver {verdict.best_solver_id}\")\n",
    "print(f\"\\nRationale:\")\n",
    "print(verdict.rationale)\n",
    "print(f\"\\nFINAL ANSWER TO USER: {verdict.final_answer_to_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation\n",
    "\n",
    "Compare the final answer against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"EVALUATION\")\n",
    "\n",
    "ground_truth = problem[\"ground_truth\"]\n",
    "\n",
    "evaluation = grade_answer(\n",
    "    client,\n",
    "    question,\n",
    "    ground_truth,\n",
    "    verdict.final_answer_to_user,\n",
    ")\n",
    "\n",
    "print(f\"\\nGround Truth: {ground_truth}\")\n",
    "print(f\"System Answer: {verdict.final_answer_to_user}\")\n",
    "print(f\"\\nCorrect: {'YES' if evaluation.is_correct else 'NO'}\")\n",
    "print(f\"\\nEvaluation Reasoning:\")\n",
    "print(evaluation.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header(\"DEBATE SUMMARY\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Problem ID: {problem['id']}\n",
    "Category: {problem['category']}\n",
    "Difficulty: {problem['difficulty']}\n",
    "\n",
    "Roles:\n",
    "  - Judge: Agent {judge_id}\n",
    "  - Solvers: {', '.join(solver_ids)}\n",
    "\n",
    "Initial Answers:\n",
    "\"\"\")\n",
    "for sid in solver_ids:\n",
    "    print(f\"  - Solver {sid}: {initial_solutions[sid].final_answer}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Refined Answers:\n",
    "\"\"\")\n",
    "for sid in solver_ids:\n",
    "    changed = \"(changed)\" if initial_solutions[sid].final_answer != refined_solutions[sid].final_answer else \"\"\n",
    "    print(f\"  - Solver {sid}: {refined_solutions[sid].final_answer} {changed}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Judge Selected: Solver {verdict.best_solver_id}\n",
    "Final Answer: {verdict.final_answer_to_user}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "Result: {'CORRECT' if evaluation.is_correct else 'INCORRECT'}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Run: Full Debate Pipeline\n",
    "\n",
    "Use this cell to run a complete debate in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_debate(problem_id: int) -> dict:\n",
    "    \"\"\"Run a complete debate for a given problem ID.\"\"\"\n",
    "    problem = next((p for p in problems if p[\"id\"] == problem_id), None)\n",
    "    if not problem:\n",
    "        raise ValueError(f\"Problem ID {problem_id} not found\")\n",
    "    \n",
    "    question = problem[\"question\"]\n",
    "    print(f\"Running debate for Problem {problem_id}: {question[:80]}...\")\n",
    "    \n",
    "    # Stage 0: Role assignment\n",
    "    preferences = [get_role_preference(client, aid, question) for aid in PERSONAS.keys()]\n",
    "    judge_candidates = [p for p in preferences if p.role_priority == \"Judge\"]\n",
    "    if judge_candidates:\n",
    "        judge_id = random.choices(judge_candidates, weights=[p.confidence for p in judge_candidates], k=1)[0].agent_id\n",
    "    else:\n",
    "        judge_id = random.choice(preferences).agent_id\n",
    "    solver_ids = [p.agent_id for p in preferences if p.agent_id != judge_id]\n",
    "    print(f\"  Roles: Judge={judge_id}, Solvers={solver_ids}\")\n",
    "    \n",
    "    # Stage 1: Solutions\n",
    "    initial_solutions = {sid: generate_solution(client, sid, question) for sid in solver_ids}\n",
    "    print(f\"  Initial answers: {[initial_solutions[s].final_answer[:30] for s in solver_ids]}\")\n",
    "    \n",
    "    # Stage 2: Reviews\n",
    "    all_reviews = {sid: [] for sid in solver_ids}\n",
    "    for reviewer in solver_ids:\n",
    "        for target in solver_ids:\n",
    "            if reviewer != target:\n",
    "                all_reviews[target].append(\n",
    "                    generate_critique(client, reviewer, target, question, initial_solutions[target])\n",
    "                )\n",
    "    print(f\"  Peer reviews completed\")\n",
    "    \n",
    "    # Stage 3: Refinement\n",
    "    refined_solutions = {\n",
    "        sid: refine_solution(client, sid, question, initial_solutions[sid], all_reviews[sid])\n",
    "        for sid in solver_ids\n",
    "    }\n",
    "    print(f\"  Refined answers: {[refined_solutions[s].final_answer[:30] for s in solver_ids]}\")\n",
    "    \n",
    "    # Stage 4: Verdict\n",
    "    verdict = judge_verdict(client, judge_id, question, solver_ids, initial_solutions, all_reviews, refined_solutions)\n",
    "    print(f\"  Judge selected: Solver {verdict.best_solver_id}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation = grade_answer(client, question, problem[\"ground_truth\"], verdict.final_answer_to_user)\n",
    "    print(f\"  Final answer: {verdict.final_answer_to_user}\")\n",
    "    print(f\"  Ground truth: {problem['ground_truth']}\")\n",
    "    print(f\"  Result: {'CORRECT' if evaluation.is_correct else 'INCORRECT'}\")\n",
    "    \n",
    "    return {\n",
    "        \"problem\": problem,\n",
    "        \"verdict\": verdict,\n",
    "        \"evaluation\": evaluation,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Run on problem 7 (Monty Hall)\n",
    "# result = run_full_debate(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
